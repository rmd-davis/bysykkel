{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmd-davis/bysykkel/blob/main/bysykkel_adventure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NedrY8bKDt1h"
      },
      "source": [
        "# Bysykkel i Trondheim\n",
        "\n",
        "Du skal være konsulent for en dag, og din første kunde er Trondheim Bysykkel (et samarbeid mellom Trondheim Kommune og Clear Channel Norway AS).\n",
        "\n",
        "Trondheim Kommune ønsker å tilby elsykler (i stedet for vanlige sykler) på de strekningene hvor folk bruker mest tid fra A til B, men de vet ikke hvilke strekgninger det gjelder. De ønsker denne informasjonen i både tabulær format og visualisert i et kart. \n",
        "\n",
        "De har gitt deg et Excel ark med ~140,000 turer fra 2021, og regner med at du klarer det på noen dager. Heldigvis kan du litt Pandas, og vet at du får til dette mye raskere.\n",
        "\n",
        "Pandas docs: https://pandas.pydata.org/docs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2OKH85my8Cm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQSVjNw9Dt1k"
      },
      "outputs": [],
      "source": [
        "# Install packages needed for tutorial\n",
        "# Restart runtime after running this cell\n",
        "\n",
        "#!pip uninstall pandas-profiling\n",
        "!pip install pandas-profiling==3.1.0 -q --no-warn-conflicts\n",
        "!pip install pydeck -q --no-warn-conflicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ACg-_hy-hM"
      },
      "outputs": [],
      "source": [
        "# Clone the bysykkel github repo into colab\n",
        "!git clone https://github.com/rmd-davis/bysykkel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWpw7B8-zDjT"
      },
      "outputs": [],
      "source": [
        "# Change directory into the bysykkel repository\n",
        "%cd bysykkel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8y-aDU60kjQ"
      },
      "outputs": [],
      "source": [
        "# Load required packages\n",
        "\n",
        "# Pandas for handling data\n",
        "import pandas as pd\n",
        "\n",
        "# Pandas profiling for automated exploratory data analysis\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "# Pydeck for mapping\n",
        "import pydeck as pdk\n",
        "\n",
        "# Matplotlib and pyplot for plotting\n",
        "\n",
        "from matplotlib import pyplot as plt \n",
        "import seaborn as sns\n",
        "\n",
        "# Import script for code testing\n",
        "# This is only needed to test the codeblocks you write in this tutorial \n",
        "from functions import codetests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ2A-TnpDt1k"
      },
      "source": [
        "## Last inn data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8nIC4puDt1m"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_excel('data/bysykkel_2021.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjUPGNjCDt1m"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Inspect the data we loaded using df.head().\n",
        "Note that the default setting for df.head() is to return the first 5 rows.\n",
        "By specifying df.head(10), we get the first 10 rows instead.\n",
        "'''\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9KNQiBfDt1o"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Looks like we have some cleaning up to do!\n",
        "In the Excel file, there is some other information at the top before the data starts on the 7th row.\n",
        "Load data using pd.read_excel, but skip the first 6 rows and set the header row using the header parameter\n",
        "'''\n",
        "\n",
        "### Enter your code below this line\n",
        "\n",
        "\n",
        "\n",
        "### Enter your code above this line\n",
        "\n",
        "\n",
        "### Code test\n",
        "codetest_01_input = df.iloc[0,0]\n",
        "codetests.codetest_01_header(codetest_01_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQeEJJkgDt1p"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Inspect data after adding the header parameter to the load function. \n",
        "If your code passed codetest_01, the first row should contain a trip from Station 118 to Station 108\n",
        "'''\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwEeJS4wDt1p"
      },
      "source": [
        "## Automatic EDA (Exploratory Data Analysis)\n",
        "\n",
        "Now that we have the dataset loaded, we can use Pandas Profiling to get some quick insights into the dataset.\n",
        "\n",
        "https://pypi.org/project/pandas-profiling/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM7nLnuoDt1q"
      },
      "outputs": [],
      "source": [
        "# Generate Pandas Profiling report\n",
        "profile = ProfileReport(df, title='Bysykkel Trondheim', html={'style':{'full_width':True}})\n",
        "profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKmQJAn_Dt1r"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Use df.info() to get basic information about the dataset. \n",
        "Notice the Dtype for 'started_at' and 'ended_at'.\n",
        "Also, notice the number of non-null values in 'ended_at', 'start_station_description', and 'end_station_description'\n",
        "'''\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO9gh8rUDt1q"
      },
      "source": [
        "## Data cleanup\n",
        "\n",
        "Issues with the dataset:\n",
        "\n",
        "* 'started_at' and 'ended_at' are strings and not datetime variables\n",
        "* Missing 'start_station_name'\n",
        "* There are some extra columns we don't need\n",
        "* There are duplicate rows\n",
        "* Missing duration for each trip\n",
        "* 'ended_at' has 703 missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGr1O4v4Dt1q"
      },
      "source": [
        "### Dates\n",
        "\n",
        "'started_at' and 'ended_at' are 'objects', but should be 'datetime'\n",
        "<br>https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOCNLGYeDt1r"
      },
      "outputs": [],
      "source": [
        "''' \n",
        "Convert 'started_at' and 'ended_at' to datetime variables.\n",
        "Use pd.to_datetime()\n",
        "To apply a transformation to a particular column, you can use the convention below:\n",
        "df['column_name'] = transformation(df['column_name'])\n",
        "\n",
        "'''\n",
        "\n",
        "### Enter your code below this line\n",
        "\n",
        "\n",
        "\n",
        "### Enter your code above this line\n",
        "\n",
        "\n",
        "### Code test\n",
        "codetest_02_input = df.dtypes\n",
        "codetests.codetest_02_dtypes(codetest_02_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3us76iO9Dt1r"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Use df.info() again, and now notice the Dtype for 'started_at' and 'ended_at'.\n",
        "If you passed the codetest above, they will now both be datetime variables.\n",
        "'''\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeqYbycZDt1s"
      },
      "source": [
        "### Missing data\n",
        "We are missing the 'start_station_name' column to match the 'end_station_name' column.\n",
        "<br>Luckily, we have another Excel file that lists station IDs and station names. We can combine the information from both files using df.merge()\n",
        "<br>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3LjmzsMDt1s"
      },
      "outputs": [],
      "source": [
        "# Load 'start_station_id.xlsx' into a new datadrame called 'station_names'\n",
        "\n",
        "station_names = pd.read_excel('data/start_station_id.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAu3zMcIDt1s"
      },
      "outputs": [],
      "source": [
        "# Inspect station_names dataframe\n",
        "station_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QehwxL9KCLL0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Now we will merge the original dataframe and the information in the station_names dataframe.\n",
        "Use df.merge, merge on 'start_station_id', and use a 'left' merge.\n",
        "'''\n",
        "\n",
        "### Enter your code below this line\n",
        "\n",
        "\n",
        "\n",
        "### Enter your code above this line\n",
        "\n",
        "\n",
        "### Code test\n",
        "codetest_03_input = df\n",
        "codetests.codetest_03_merge(codetest_03_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEMFrtdADt1t"
      },
      "source": [
        "### Delete unnecessary columns\n",
        "\n",
        "Delete 'start_station_description' and 'end_station_description'. \n",
        "<br>Use df.drop( ).\n",
        "<br> https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1aiZVNzDt1t"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Delete columns using df.drop()\n",
        "You can either assign the result of df.drop to df, or use the inplace parameter\n",
        "'''\n",
        "\n",
        "### Enter your code below this line\n",
        "\n",
        "\n",
        "\n",
        "### Enter your code above this line\n",
        "\n",
        "\n",
        "### Code test\n",
        "codetest_04_input = df\n",
        "codetests.codetest_04_drop(codetest_04_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XGzN093Dt1t"
      },
      "source": [
        "### Remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv47nKYXDt1t"
      },
      "outputs": [],
      "source": [
        "# First, we find the duplicated rows using df.duplicated()\n",
        "\n",
        "df[df.duplicated(keep=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffz65wO7Dt1u"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Now we will use df.drop() to drop a single row from the dataframe.\n",
        "We will drop the row with index = 29143.\n",
        "Note that you can specify which axis to drop, but it is not necessary since the default behaviour is axis=0, which is what we want to drop a row.\n",
        "'''\n",
        "\n",
        "### Enter your code below this line\n",
        "\n",
        "\n",
        "\n",
        "### Enter your code above this line\n",
        "\n",
        "\n",
        "### Code test\n",
        "codetest_05_input = df\n",
        "codetests.codetest_05_duplicates(codetest_05_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW4kSnwvDt1u"
      },
      "source": [
        "### Add a column with trip duration\n",
        "Add a column which contains the duration of each trip in seconds.\n",
        "<br> Since we have 'started_at' and 'ended_at', Pandas can do the calculation for us automatically as a timedelta. \n",
        "<br>We'll use <code>df[column].dt.seconds</code> to get seconds as a float.\n",
        "\n",
        "<br> Instead of you coding this one, we will just show you the power and time savings of using a vectorized approach to calculations. We'll use the <code>%%time</code> command to time different ways of performing the calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUrLt-MPDt1u"
      },
      "outputs": [],
      "source": [
        "# Calculating duration by looping through each row iteratively.\n",
        "# After each iteration, we'll build up a list of duration values\n",
        "# After looping through all rows, we add the list of duration values to the dataframe\n",
        "\n",
        "%%time\n",
        "\n",
        "duration_list = []\n",
        "duration_value = 0\n",
        "\n",
        "for row in df.index:\n",
        "    duration_value = (df.loc[row]['ended_at'] - df.loc[row]['started_at']).seconds\n",
        "    duration_list.append(duration_value)\n",
        "\n",
        "df['duration_loop'] = duration_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7X9JbPRDt1u"
      },
      "outputs": [],
      "source": [
        "# Calculating duration using a lambda function\n",
        "# Here we build the duration column as we go\n",
        "\n",
        "%%time\n",
        "\n",
        "df['duration_lambda'] = df.apply(lambda x: (x['ended_at'] - x['started_at']).seconds, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0IKsDEdDt1u"
      },
      "outputs": [],
      "source": [
        "# Finally, we'll look at a vectorized approach\n",
        "%%time\n",
        "\n",
        "# Calculate timedelta between 'ended_at' and 'started_at'\n",
        "df['duration'] = df['ended_at'] - df['started_at']\n",
        "\n",
        "# Convert column from timedelta to float of seconds\n",
        "df['duration'] = df['duration'].dt.seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwOfVM_QDt1v"
      },
      "source": [
        "### Fix missing values in 'ended_at'\n",
        "* There are many ways of fixing missing values\n",
        "* The most important thing is to document what you are changing in your dataset, and how you are changing it.\n",
        "* We will fill the missing values in 'duration' with the average trip duration, generate a column of timedeltas, and then use that to fix all ended_at values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSpaOx9JDt1w"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in the 'duration' column with the mean of all duration values\n",
        "df['duration'].fillna(df['duration'].mean(), inplace=True)\n",
        "\n",
        "# Create a timedelta column based on the duration column\n",
        "# Timedeltas are used to make calculations based on variables in a datetime format, so we need to convert our duration column which is stored as a float\n",
        "df['duration_timedelta'] = pd.to_timedelta(df['duration'], unit='s')\n",
        "\n",
        "# Update all 'ended_at' values\n",
        "df['ended_at'] = df['started_at'] + df['duration_timedelta']\n",
        "\n",
        "# Drop the columns we no longer need in the dataframe\n",
        "df = df.drop(columns = ['duration_timedelta','duration_loop','duration_lambda'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3D4fe9Dt1w"
      },
      "source": [
        "\n",
        "## Check the result of the data cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6uFDZRUUYZY"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOZl8jTVDt1w"
      },
      "outputs": [],
      "source": [
        "# Use df.describe() or df.describe().transpose()\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta2Kk3lyDt1w"
      },
      "source": [
        "## Plot the longest trip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE0qcBOaDt1w"
      },
      "outputs": [],
      "source": [
        "# Select the n longest trips\n",
        "\n",
        "# Set the number of trips you want to look at.\n",
        "# Plot just the 1 longest trip first. Then change it to the 20 longest trips.\n",
        "n_trips = 1\n",
        "\n",
        "# Create a new dataframe with only the values we want to plot\n",
        "df_map_data = df.sort_values(by='duration', na_position='first').tail(n_trips)\n",
        "\n",
        "# Add columns for start coordinates and end coordinates in the format [long,lat]\n",
        "df_map_data['start_coord'] = df_map_data.apply(lambda x: [x['start_station_longitude'],x['start_station_latitude']],axis=1)\n",
        "df_map_data['end_coord'] = df_map_data.apply(lambda x: [x['end_station_longitude'],x['end_station_latitude']], axis=1)\n",
        "\n",
        "# Generate the map\n",
        "layer = pdk.Layer(\"LineLayer\", df_map_data, get_source_position=\"start_coord\",get_target_position=\"end_coord\", get_color = [255,0,0], get_width = 3)\n",
        "init_view_state = pdk.ViewState(longitude=10.3985, latitude=63.4256, zoom=12)\n",
        "r = pdk.Deck(layers=layer, initial_view_state=init_view_state, map_style='light')\n",
        "r.to_html();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlff4eGCSjyz"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6CRz1H-o1Fp"
      },
      "source": [
        "# Other useful Pandas functions\n",
        "\n",
        "https://pandas.pydata.org/docs/\n",
        "\n",
        "The fastest way to solve a problem is usually through Google or StackOverflow.If you have a question, it's highly likely that it has already been asked and solved on StackOverflow.\n",
        "\n",
        "There are TONS of articles with 'Top n Pandas Functions you must know'. Here's a selection of some of the functions we have use of in projects.\n",
        "\n",
        "Remember that a lot of operations that can be done on a whole dataframe can also be done on a single column as a Series.\n",
        "\n",
        "```\n",
        "df.method() # Accesses entire Pandas DataFrame\n",
        "df['column_name'].method() # Accesses only the column as a Pandas Series\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "- style (precision, conditional formatting, etc)\n",
        "- pd.set_option() - https://pandas.pydata.org/docs/user_guide/options.html?highlight=options%20display\n",
        "- df.nlargest og df.nsmallest - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nlargest.html?highlight=nlargest#pandas.DataFrame.nlargest\n",
        "- df.idxmax() og df.idxmin() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html?highlight=idxmax#pandas.DataFrame.idxmax\n",
        "- df.count() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.count.html?highlight=count#pandas.DataFrame.count\n",
        "- df.value_counts(dropna = False) - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html?highlight=value_counts#pandas.DataFrame.value_counts\n",
        "- df.at_time og df.between_time - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at_time.html?highlight=at_time\n",
        "- df.replace() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html?highlight=replace#pandas.DataFrame.replace\n",
        "- df.fillna() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html?highlight=fillna#pandas.DataFrame.fillna\n",
        "- df.groupby() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html?highlight=groupby#pandas.DataFrame.groupby\n",
        "- df.query() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query\n",
        "- df.where() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html?highlight=where#pandas.DataFrame.where\n",
        "- df['column_name'].isin() - https://pandas.pydata.org/docs/reference/api/pandas.Series.isin.html?highlight=isin#pandas.Series.isin\n",
        "- df['column_name'].unique() - https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html?highlight=unique#pandas.Series.unique\n",
        "- df['column_name'].nunique() - https://pandas.pydata.org/docs/reference/api/pandas.Series.nunique.html?highlight=nunique#pandas.Series.nunique\n",
        "- df['column_name'].cumsum() - https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html?highlight=cumsum#pandas.Series.cumsum\n",
        "- df['column_name'].pct_change() - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html?highlight=pct%20change#pandas.DataFrame.pct_change\n",
        "- df['column_name'].shift() - https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html?highlight=shift#pandas.Series.shift\n",
        "- Windowing operations - https://pandas.pydata.org/docs/user_guide/window.html#window-expanding\n",
        "\n",
        "\n",
        "- df.pipe - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html?highlight=pipe#pandas.DataFrame.pipe \n",
        "- Using .pkl to compress CSV files - https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html \n",
        "- Handling larger amounts of data - https://pandas.pydata.org/docs/user_guide/scale.html\n",
        "- Pandas Cookbook - https://pandas.pydata.org/docs/user_guide/cookbook.html\n",
        "- Built-in plotting - https://pandas.pydata.org/docs/user_guide/visualization.html\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRNlyHsXSRHB"
      },
      "source": [
        "## Bonus: Alternate strategy for filling missing durations\n",
        "Using averages for each segment to fill values for those segments.\n",
        "\n",
        "Note that this will not produce a result now since the code in '*Fix missing values in 'ended_at' '* has already cleaned up all the missing values in the 'ended_at' column.\n",
        "BUT, all of this code below could replace the code in *'Fix missing values in 'ended_at''*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og3TOQU7SSV3"
      },
      "outputs": [],
      "source": [
        "# Make unique segment ids based on start station and end station\n",
        "df['segment'] = df['start_station_id'].astype(str) + '_' + df['end_station_id'].astype(str)\n",
        "\n",
        "unique_segments = df['segment'].unique()\n",
        "\n",
        "print(f'Total number of unique segments: {len(unique_segments)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY0ClxZ9SvFm"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of segment names and mean duration for each segment\n",
        "segment_means = {}\n",
        "\n",
        "# Loop over all of the unique segments and calculate the mean duration for that segment based on all trips for that segment\n",
        "\n",
        "for segment in unique_segments:\n",
        "    temp_frame = df[df['segment'] == segment]\n",
        "    segment_means[segment] = temp_frame['duration'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMRfa7u-Szo-"
      },
      "outputs": [],
      "source": [
        "# Extract dataframe with missing ended_at values\n",
        "# Use df.isna()\n",
        "\n",
        "missing_endtime = df[df['ended_at'].isna()]\n",
        "missing_endtime = missing_endtime[['started_at','segment']]\n",
        "\n",
        "# Show missing_endtime dataframe\n",
        "missing_endtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPzD7qtES1j-"
      },
      "outputs": [],
      "source": [
        "# Fill in missing durations with values from segment_means dictionary\n",
        "\n",
        "missing_endtime['duration'] = missing_endtime['segment'].apply(lambda x: segment_means[x]) \n",
        "\n",
        "# Create timedelta, needed for calculation of ended_at\n",
        "missing_endtime['duration_dt'] = pd.to_timedelta(missing_endtime['duration'], unit='s')\n",
        "\n",
        "# Calculate ended_at\n",
        "missing_endtime['ended_at'] = missing_endtime['started_at'] + missing_endtime['duration_dt']\n",
        "\n",
        "# Extract necessary columns, needed to fill original dataframe\n",
        "missing_endtime = missing_endtime[['ended_at','duration']]\n",
        "\n",
        "# Show updated dataframe\n",
        "missing_endtime\n",
        "\n",
        "\n",
        "# # Fill missing values in original dataframe based on values in missing_endtime dataframe\n",
        "# # df.fillna() will automatically match based on index to fill in 'ended_at' and 'duration' values\n",
        "\n",
        "# df = df.fillna(missing_endtime)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "bysykkel_adventure.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "09dc4478fbc8c738def0b0030cd85816df6418367d204095b9d48e2eff823494"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
